---
title: "Image Classification using CNN"
author: "Sharmishta"
output: html_document
---

<br>

**1. Install the required libraries**

```{r}
#devtools::install_github("rstudio/tensorflow", force = TRUE)
#devtools::install_github("rstudio/keras", force = TRUE)

#install_tensorflow()
#install_keras()
```
<br>

**2. Add the libraries in the R chunk above.**

```{r}
library(kableExtra)
library(ggplot2) 
library(tensorflow)
library(keras)
library(tfruns)
library(dplyr)
library(mlbench)
library(psych)
library(magrittr)
library(neuralnet)
```
<br>

**3. For this assignment, we will be using mnist data set.**
<br>

**The following chunk calls the data:**

```{r}
mnist=dataset_mnist()

x_train=mnist$train$x
y_train=mnist$train$y
x_test=mnist$test$x
y_test=mnist$test$y
```
<br>

## Questions
<br>

**4. Set up a convolutional neural network model with at least 2 conv. levels. Explain why you choose each parameter and layer.**
<br>

```{r}
x_train <- array_reshape(x_train, c(nrow(x_train), 28, 28, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), 28, 28, 1))

x_train <- x_train / 255
x_test <- x_test / 255

y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
```
<br>

```{r}
model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu", input_shape = c(28, 28, 1)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten() %>%
  layer_dense(units = 128, activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 10, activation = "softmax")
```
<br>

```{r}
summary(model)
```
<br>

We decided to start with 2 convoluted layers and 2 dense layers and increase the number of layers if needed. The first convolutional layer has 32 filters and a kernel size of 3x3, and the second convolutional layer has 64 filters and a kernel size of 3x3. The input size here is (28x28x1) as it is the size of the images. In both convolutional layers, we used the ReLU activation function to introduce non-linearity. We also added max pooling layers after each convolutional layer, to reduce the spatial dimensions of the output and help to avoid overfitting. Then, we added last two dense layers with 128 units and 10 units respectively, and a dropout layer to prevent overfitting.
<br>
<br>

**5. Compile the model. Explain why you chose a particular optimizer, loss, and evaluation metric.**
<br>

```{r}
model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_adam(learning_rate = 0.001),
  metrics = c('accuracy')
)
```
<br>

We chose Adam optimizer as our optimizer as it is commonly used. And for the loss and metrics, we used categorical cross-entropy and accuracy as we used the same in our first class. The learning rate for the optimizer was decided arbitrarily as 0.001
<br>
<br>


**6. Fit the model. Explain why you chose the model parameters.**

```{r}
history <- model %>% fit(
  x_train, y_train,
  epochs = 10,
  batch_size = 128,
  validation_data = list(x_test, y_test)
)
```
<br>

Since, our dataset is really big, we used less epochs as we don't have much computational power in our devices. And batch size is chosen as 128 arbitrarily.
<br>
<br>

**7. Evaluate and predict your model. Can you plot the predicted vs actual? **

```{r}
model %>% evaluate(x_test, y_test, verbose = 0)
```
<br>


```{r}
y_pred <- model %>%
  predict(x_test)
```
<br>

```{r}
y_test_new=array(0,c(10000))
pred_new = array(0,c(10000))


for (i in 1:10000){
  for(j in 1:10){
    if(y_test[i,j]==1){
      y_test_new[i]=j-1
    }
  }
}

for (i in 1:10000){
  for(j in 1:10){
    if(y_pred[i,j]==1){
      pred_new[i]=j-1
    }
  }
}

plot(y_test_new,col="gray", ylim = c(0,10), main = "Predicted (red) vs. Actual (gray)", ylab = "Predicted number")
points(pred_new,col="red")
```